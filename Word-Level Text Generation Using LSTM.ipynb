{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348e932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as optim\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c794e1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Japanese', 'growth', 'grinds', 'to', 'a', 'halt', 'Growth', 'in', 'Japan', 'evaporated', 'the', 'three', 'months', 'September,', 'sparking', 'renewed', 'concern', 'about', 'an', 'economy', 'not', 'long', 'out', 'of', 'decade-long', 'trough.', 'Output', 'period', 'grew', 'just', '0.1%,', 'annual', 'rate', '0.3%.', 'Exports', '-', 'usual', 'engine', 'recovery', 'faltered,', 'while', 'domestic', 'demand', 'stayed', 'subdued', 'and', 'corporate', 'investment', 'also', 'fell', 'short.', 'The', 'falls', 'well', 'short', 'expectations,', 'but', 'does', 'mark', 'sixth', 'straight', 'quarter', 'expansion.', 'had', 'stagnated', 'throughout', '1990s,', 'experiencing', 'only', 'brief', 'spurts', 'expansion', 'amid', 'periods', 'doldrums.', 'One', 'result', 'was', 'deflation', 'prices', 'falling', 'rather', 'than', 'rising', 'which', 'made', 'shoppers', 'cautious', 'kept', 'them', 'from', 'spending.', 'effect', 'leave', 'more', 'dependent', 'ever', 'on', 'exports', 'for', 'its', 'recent', 'recovery.', 'But', 'high', 'oil', 'have', 'knocked', '0.2%', 'off', 'rate,', 'dollar', 'means', 'products', 'shipped', 'US', 'are', 'becoming', 'relatively', 'expensive.', 'performance', 'third', 'marks', 'sharp', 'downturn', 'earlier', 'year.', 'first', 'showed', '6.3%,', 'with', 'second', 'showing', '1.1%,', 'economists', 'been', 'predicting', 'as', 'much', '2%', 'this', 'time', 'around.', '\"Exports', 'slowed', 'capital', 'spending', 'became', 'weaker,\"', 'said', 'Hiromichi', 'Shirakawa,', 'chief', 'economist', 'at', 'UBS', 'Securities', 'Tokyo.', '\"Personal', 'consumption', 'looks', 'good,', 'it', 'mainly', 'due', 'temporary', 'factors', 'such', 'Olympics.', '\"The', 'amber', 'light', 'is', 'flashing.\"', 'government', 'may', 'now', 'find', 'difficult', 'raise', 'taxes,', 'policy', 'will', 'implement', 'when', 'picks', 'up', 'help', 'deal', \"Japan's\", 'massive', 'public', 'debt.']\n",
      "{'Japanese': 0, 'growth': 1, 'grinds': 2, 'to': 3, 'a': 4, 'halt': 5, 'Growth': 6, 'in': 7, 'Japan': 8, 'evaporated': 9, 'the': 10, 'three': 11, 'months': 12, 'September,': 13, 'sparking': 14, 'renewed': 15, 'concern': 16, 'about': 17, 'an': 18, 'economy': 19, 'not': 20, 'long': 21, 'out': 22, 'of': 23, 'decade-long': 24, 'trough.': 25, 'Output': 26, 'period': 27, 'grew': 28, 'just': 29, '0.1%,': 30, 'annual': 31, 'rate': 32, '0.3%.': 33, 'Exports': 34, '-': 35, 'usual': 36, 'engine': 37, 'recovery': 38, 'faltered,': 39, 'while': 40, 'domestic': 41, 'demand': 42, 'stayed': 43, 'subdued': 44, 'and': 45, 'corporate': 46, 'investment': 47, 'also': 48, 'fell': 49, 'short.': 50, 'The': 51, 'falls': 52, 'well': 53, 'short': 54, 'expectations,': 55, 'but': 56, 'does': 57, 'mark': 58, 'sixth': 59, 'straight': 60, 'quarter': 61, 'expansion.': 62, 'had': 63, 'stagnated': 64, 'throughout': 65, '1990s,': 66, 'experiencing': 67, 'only': 68, 'brief': 69, 'spurts': 70, 'expansion': 71, 'amid': 72, 'periods': 73, 'doldrums.': 74, 'One': 75, 'result': 76, 'was': 77, 'deflation': 78, 'prices': 79, 'falling': 80, 'rather': 81, 'than': 82, 'rising': 83, 'which': 84, 'made': 85, 'shoppers': 86, 'cautious': 87, 'kept': 88, 'them': 89, 'from': 90, 'spending.': 91, 'effect': 92, 'leave': 93, 'more': 94, 'dependent': 95, 'ever': 96, 'on': 97, 'exports': 98, 'for': 99, 'its': 100, 'recent': 101, 'recovery.': 102, 'But': 103, 'high': 104, 'oil': 105, 'have': 106, 'knocked': 107, '0.2%': 108, 'off': 109, 'rate,': 110, 'dollar': 111, 'means': 112, 'products': 113, 'shipped': 114, 'US': 115, 'are': 116, 'becoming': 117, 'relatively': 118, 'expensive.': 119, 'performance': 120, 'third': 121, 'marks': 122, 'sharp': 123, 'downturn': 124, 'earlier': 125, 'year.': 126, 'first': 127, 'showed': 128, '6.3%,': 129, 'with': 130, 'second': 131, 'showing': 132, '1.1%,': 133, 'economists': 134, 'been': 135, 'predicting': 136, 'as': 137, 'much': 138, '2%': 139, 'this': 140, 'time': 141, 'around.': 142, '\"Exports': 143, 'slowed': 144, 'capital': 145, 'spending': 146, 'became': 147, 'weaker,\"': 148, 'said': 149, 'Hiromichi': 150, 'Shirakawa,': 151, 'chief': 152, 'economist': 153, 'at': 154, 'UBS': 155, 'Securities': 156, 'Tokyo.': 157, '\"Personal': 158, 'consumption': 159, 'looks': 160, 'good,': 161, 'it': 162, 'mainly': 163, 'due': 164, 'temporary': 165, 'factors': 166, 'such': 167, 'Olympics.': 168, '\"The': 169, 'amber': 170, 'light': 171, 'is': 172, 'flashing.\"': 173, 'government': 174, 'may': 175, 'now': 176, 'find': 177, 'difficult': 178, 'raise': 179, 'taxes,': 180, 'policy': 181, 'will': 182, 'implement': 183, 'when': 184, 'picks': 185, 'up': 186, 'help': 187, 'deal': 188, \"Japan's\": 189, 'massive': 190, 'public': 191, 'debt.': 192}\n",
      "{0: 'Japanese', 1: 'growth', 2: 'grinds', 3: 'to', 4: 'a', 5: 'halt', 6: 'Growth', 7: 'in', 8: 'Japan', 9: 'evaporated', 10: 'the', 11: 'three', 12: 'months', 13: 'September,', 14: 'sparking', 15: 'renewed', 16: 'concern', 17: 'about', 18: 'an', 19: 'economy', 20: 'not', 21: 'long', 22: 'out', 23: 'of', 24: 'decade-long', 25: 'trough.', 26: 'Output', 27: 'period', 28: 'grew', 29: 'just', 30: '0.1%,', 31: 'annual', 32: 'rate', 33: '0.3%.', 34: 'Exports', 35: '-', 36: 'usual', 37: 'engine', 38: 'recovery', 39: 'faltered,', 40: 'while', 41: 'domestic', 42: 'demand', 43: 'stayed', 44: 'subdued', 45: 'and', 46: 'corporate', 47: 'investment', 48: 'also', 49: 'fell', 50: 'short.', 51: 'The', 52: 'falls', 53: 'well', 54: 'short', 55: 'expectations,', 56: 'but', 57: 'does', 58: 'mark', 59: 'sixth', 60: 'straight', 61: 'quarter', 62: 'expansion.', 63: 'had', 64: 'stagnated', 65: 'throughout', 66: '1990s,', 67: 'experiencing', 68: 'only', 69: 'brief', 70: 'spurts', 71: 'expansion', 72: 'amid', 73: 'periods', 74: 'doldrums.', 75: 'One', 76: 'result', 77: 'was', 78: 'deflation', 79: 'prices', 80: 'falling', 81: 'rather', 82: 'than', 83: 'rising', 84: 'which', 85: 'made', 86: 'shoppers', 87: 'cautious', 88: 'kept', 89: 'them', 90: 'from', 91: 'spending.', 92: 'effect', 93: 'leave', 94: 'more', 95: 'dependent', 96: 'ever', 97: 'on', 98: 'exports', 99: 'for', 100: 'its', 101: 'recent', 102: 'recovery.', 103: 'But', 104: 'high', 105: 'oil', 106: 'have', 107: 'knocked', 108: '0.2%', 109: 'off', 110: 'rate,', 111: 'dollar', 112: 'means', 113: 'products', 114: 'shipped', 115: 'US', 116: 'are', 117: 'becoming', 118: 'relatively', 119: 'expensive.', 120: 'performance', 121: 'third', 122: 'marks', 123: 'sharp', 124: 'downturn', 125: 'earlier', 126: 'year.', 127: 'first', 128: 'showed', 129: '6.3%,', 130: 'with', 131: 'second', 132: 'showing', 133: '1.1%,', 134: 'economists', 135: 'been', 136: 'predicting', 137: 'as', 138: 'much', 139: '2%', 140: 'this', 141: 'time', 142: 'around.', 143: '\"Exports', 144: 'slowed', 145: 'capital', 146: 'spending', 147: 'became', 148: 'weaker,\"', 149: 'said', 150: 'Hiromichi', 151: 'Shirakawa,', 152: 'chief', 153: 'economist', 154: 'at', 155: 'UBS', 156: 'Securities', 157: 'Tokyo.', 158: '\"Personal', 159: 'consumption', 160: 'looks', 161: 'good,', 162: 'it', 163: 'mainly', 164: 'due', 165: 'temporary', 166: 'factors', 167: 'such', 168: 'Olympics.', 169: '\"The', 170: 'amber', 171: 'light', 172: 'is', 173: 'flashing.\"', 174: 'government', 175: 'may', 176: 'now', 177: 'find', 178: 'difficult', 179: 'raise', 180: 'taxes,', 181: 'policy', 182: 'will', 183: 'implement', 184: 'when', 185: 'picks', 186: 'up', 187: 'help', 188: 'deal', 189: \"Japan's\", 190: 'massive', 191: 'public', 192: 'debt.'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset Preparation\n",
    "with open('business_2.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Tokenize text into word\n",
    "words = text.split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "vocab = list(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n",
    "\n",
    "print(vocab)\n",
    "print(word_to_int)\n",
    "print(int_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f75641",
   "metadata": {},
   "source": [
    "Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c558862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, samples, word_to_int):\n",
    "        self.samples = samples\n",
    "        self.word_to_int = word_to_int\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
    "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd4d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9,  7, 10, 11, 12,  3, 13, 14, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 23,  4, 24, 25, 26,  7, 10, 27, 28, 29, 30, 18,\n",
      "        31, 32, 23, 33, 34, 35, 10, 36, 37, 23, 38, 35, 39, 40, 41, 42, 43, 44,\n",
      "        45, 46, 47, 48, 49, 50, 51,  1, 52, 53]), tensor([ 2,  3,  4,  5,  6,  7,  8,  9,  7, 10, 11, 12,  3, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23,  4, 24, 25, 26,  7, 10, 27, 28, 29, 30, 18, 31,\n",
      "        32, 23, 33, 34, 35, 10, 36, 37, 23, 38, 35, 39, 40, 41, 42, 43, 44, 45,\n",
      "        46, 47, 48, 49, 50, 51,  1, 52, 53, 54]))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = TextDataset(samples, word_to_int)\n",
    "dataloader = DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        shuffle = True)\n",
    "\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4a8ef",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1097fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emdedding_dim, hidden_size, num_layers):\n",
    "        super(TextGenerationLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size = emdedding_dim,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden == None:\n",
    "            hidden = self.init_hidden(x.shape[0])\n",
    "        x = self.emdedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, (h_n, c_n)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed15a3",
   "metadata": {},
   "source": [
    "Training HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76048de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "embedding_dim = 16\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b269ed49",
   "metadata": {},
   "source": [
    "Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TextGenerationLSTM(vocab_size,\n",
    "                          embedding_dim,\n",
    "                          hidden_size,\n",
    "                          num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameter(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
